{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0803dc84",
   "metadata": {},
   "source": [
    "# Data Cleaning and Natural Catastrophe detection\n",
    "\n",
    "- This notebook concentrates on how cleaning and Nat-Cat event detection is done and generates a file containing clean lemmatised title, which can be used in next steps for Feature engineering and Clustering.\n",
    "- Also cleaning all the raw data through notebook is time consuming, therefore I have created a python file with multiprocessing, available in scripts folder and used to clean the raw data by following the steps explained in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d36836",
   "metadata": {},
   "source": [
    "Steps of cleaning and Nat-Cat detection which will be followed in this notebook:\n",
    "1. Clean the `title` column from the data using methods in Data Cleaning category.\n",
    "2. Detect Nat-Cat titless based on requirement.\n",
    "3. Filter out Non Nat-Cat titles.\n",
    "4. Apply advanced cleaning on Nat-cat titles. (Location removal, stopwords removal, lemmatisation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1aef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c16c9c20",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a1494",
   "metadata": {},
   "source": [
    "pip install all the requirements available in `requirements.txt` file and also run below to download all the required datasets and models, before importing the libraries\n",
    "```\n",
    "nltk.download('all')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1fd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from spellchecker import SpellChecker\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0cf21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "775ac925",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "Data contains a lot of issues and needs to be cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e705c6b",
   "metadata": {},
   "source": [
    "### 2.1 - Remove HTML tags from the titles\n",
    "- The titles contain HTML tags like provided in the example, which need to be removed as they are not relevant to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7dd1790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The US is in the middle of an exceptional tornado streak . Here what it looks like | <span class=  tnt - section - tag no - link  >News< / span>\n",
      "Output: The US is in the middle of an exceptional tornado streak . Here what it looks like | News\n"
     ]
    }
   ],
   "source": [
    "# Removing HTML tags\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "# Title contained html tags like below <span....> needs to be removed\n",
    "text= 'The US is in the middle of an exceptional tornado streak . Here what it looks like | <span class=  tnt - section - tag no - link  >News< / span>'\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_html(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96ae45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdef00ca",
   "metadata": {},
   "source": [
    "### 2.2 - Remove any URLs and Domains from the title\n",
    "- Remove Data containing http:// or https://\n",
    "- Space-separated www URLs like www . domain . com\n",
    "- Bare domains like kfyi.iheart.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1bc6e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Sapphire Dimensional Stone - www . 2merkato . com\n",
      "Output: Sapphire Dimensional Stone -\n",
      "\n",
      "Input: kfyi.iheart.com\n",
      "Output: \n",
      "\n",
      "Input: wfyi.org\n",
      "Output: \n",
      "\n",
      "Input: http://wfyi.org\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_urls(text):\n",
    "    # Combined regex for different URL forms\n",
    "    pattern = r\"\"\"(\n",
    "        https?://\\S+ |                  # URLs starting with http:// or https://\n",
    "        www(?:\\s*\\.\\s*\\S+)+ |           # Space-separated www URLs like www . domain . com\n",
    "        \\b(?:[a-z0-9-]+\\.)+[a-z]{2,}\\b  # Bare domains like kfyi.iheart.com\n",
    "    )\"\"\"\n",
    "    \n",
    "    return re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.VERBOSE).strip()\n",
    "\n",
    "text1 = \"Sapphire Dimensional Stone - www . 2merkato . com\"\n",
    "text2 = \"kfyi.iheart.com\"\n",
    "text3 = \"wfyi.org\"\n",
    "text4 = \"http://wfyi.org\"\n",
    "\n",
    "print(\"Input: {}\".format(text1))\n",
    "print(\"Output: {}\".format(remove_urls(text1)))\n",
    "print(\"\\nInput: {}\".format(text2))\n",
    "print(\"Output: {}\".format(remove_urls(text2)))\n",
    "print(\"\\nInput: {}\".format(text3))\n",
    "print(\"Output: {}\".format(remove_urls(text3)))\n",
    "print(\"\\nInput: {}\".format(text4))\n",
    "print(\"Output: {}\".format(remove_urls(text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d6633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "347e5e5b",
   "metadata": {},
   "source": [
    "### 2.3 - Remove News Sources after pipe symbol \"|\"\n",
    "- News sources in the title are of no use for our analysis, therefor we remove them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a643635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 6 . 3 Magnitude Earthquake Reported | NewsTalk 1230\n",
      "Cleaned : 6 . 3 Magnitude Earthquake Reported\n",
      "\n",
      "Original: EAT This Week : Tahoe Tavern & Grill Heat - Check | TahoeDailyTribune . com\n",
      "Cleaned : EAT This Week : Tahoe Tavern & Grill Heat - Check\n",
      "\n",
      "Original: River Valley Early Voting Centers | The Arkansas Democrat - Gazette - Arkansa Best News Source\n",
      "Cleaned : River Valley Early Voting Centers\n",
      "\n",
      "Original: 6 . 3 Magnitude Earthquake Reported | 96 . 3 | 102 . 5 NewsRadio WFLA\n",
      "Cleaned : 6 . 3 Magnitude Earthquake Reported | 96 . 3\n",
      "\n",
      "Original: GLOBALink | China aid eases misery of quake - affected Afghans in chilly winter\n",
      "Cleaned : GLOBALink | China aid eases misery of quake - affected Afghans in chilly winter\n",
      "\n",
      "Original: UK wakes up to freezing weather as temperatures plummet to - 6 . 4C | united kingdom News\n",
      "Cleaned : UK wakes up to freezing weather as temperatures plummet to - 6 . 4C\n",
      "\n",
      "Original: 10 deadliest natural disasters that ever happened | The Times of India\n",
      "Cleaned : 10 deadliest natural disasters that ever happened\n",
      "\n",
      "Original: A14 lanes still closed near Newmarket after heavy flooding | East Anglian Daily Times\n",
      "Cleaned : A14 lanes still closed near Newmarket after heavy flooding\n",
      "\n",
      "Original: UK weather : New maps show massive 688 - mile snow bomb covering Britain from top to bottom | Weather | News\n",
      "Cleaned : UK weather : New maps show massive 688 - mile snow bomb covering Britain from top to bottom | Weather\n",
      "\n",
      "Original: Mountain Dew launches summer campaign with Hrithik Roshan | 1 Indian Television Dot Com\n",
      "Cleaned : Mountain Dew launches summer campaign with Hrithik Roshan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# List of common news-related keywords\n",
    "news_keywords = [\n",
    "    \"tribune\", \"journal\", \"gazette\", \"times\", \"post\", \"daily\", \"herald\",\n",
    "    \"observer\", \"review\", \"report\", \"sun\", \"star\", \"news\", \"press\", \"bulletin\",\n",
    "    \"the arkansas democrat\", \"tahoedailytribune\", \"nytimes\", \"ktvu\",\n",
    "    \"indian television\"  \n",
    "]\n",
    "\n",
    "# Regex pattern to match common domain endings\n",
    "# This pattern matches common domain endings like .com, .net, .org, etc.\n",
    "domain_pattern = re.compile(r'\\.\\s*(com|net|org|tv|info|co|us)\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to normalize text by removing extra spaces and converting to lowercase\n",
    "def normalize(text):\n",
    "    text = re.sub(r'\\s*\\.\\s*', '.', text.lower())\n",
    "    text = re.sub(r'[^a-z0-9. ]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Function to check if a segment is likely a news source\n",
    "def is_probably_news_source(segment):\n",
    "    norm = normalize(segment)\n",
    "    \n",
    "    if domain_pattern.search(norm):\n",
    "        return True\n",
    "    \n",
    "    # Check spelled out domains like 'dot com'\n",
    "    if re.search(r'dot\\s*(com|net|org|tv|info|co|us)\\b', norm):\n",
    "        return True\n",
    "    \n",
    "    return any(k in norm for k in news_keywords)\n",
    "\n",
    "# Function to remove news source from title\n",
    "def remove_news_source(title):\n",
    "    parts = [p.strip() for p in title.split('|')]\n",
    "    if len(parts) > 1 and is_probably_news_source(parts[-1]):\n",
    "        return ' | '.join(parts[:-1])\n",
    "    return title\n",
    "\n",
    "\n",
    "\n",
    "# Test cases\n",
    "titles = [\n",
    "    \"6 . 3 Magnitude Earthquake Reported | NewsTalk 1230\",\n",
    "    \"EAT This Week : Tahoe Tavern & Grill Heat - Check | TahoeDailyTribune . com\",\n",
    "    \"River Valley Early Voting Centers | The Arkansas Democrat - Gazette - Arkansa Best News Source\",    \"6 . 3 Magnitude Earthquake Reported | 96 . 3 | 102 . 5 NewsRadio WFLA\",\n",
    "    \"GLOBALink | China aid eases misery of quake - affected Afghans in chilly winter\",\n",
    "    \"UK wakes up to freezing weather as temperatures plummet to - 6 . 4C | united kingdom News\",\n",
    "    \"10 deadliest natural disasters that ever happened | The Times of India\",\n",
    "    \"A14 lanes still closed near Newmarket after heavy flooding | East Anglian Daily Times\",\n",
    "    \"UK weather : New maps show massive 688 - mile snow bomb covering Britain from top to bottom | Weather | News\",\n",
    "    \"Mountain Dew launches summer campaign with Hrithik Roshan | 1 Indian Television Dot Com\"\n",
    "]\n",
    "\n",
    "cleaned = [remove_news_source(t) for t in titles]\n",
    "for original, result in zip(titles, cleaned):\n",
    "    print(f\"Original: {original}\\nCleaned : {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d07063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0caea9",
   "metadata": {},
   "source": [
    "### 2.4 - Remove Structured dates from the title\n",
    "- Remove any dates in the title as they are not relevant to the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e199ed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: World Earthquake Report for Monday , 29 January 2024\n",
      "Cleaned : World Earthquake Report for\n",
      "\n",
      "Original: HAWAIIAN VOLCANO OBSERVATORY DAILY UPDATE Tuesday , January 30 , 2024 , 16 : 50 UTC \n",
      "Cleaned : HAWAIIAN VOLCANO OBSERVATORY DAILY UPDATE\n",
      "\n",
      "Original: Vanuatu Volcano Alert Bulletin nÂ°3 - Ambrym Activity ( January 31st 2024 ) - Vanuatu\n",
      "Cleaned : Vanuatu Volcano Alert Bulletin nÂ°3 - Ambrym Activity - Vanuatu\n",
      "\n",
      "Original: Indonesia , Flooding in Karawang ( West Java ) ( 31 Jan 2024 ) - Indonesia\n",
      "Cleaned : Indonesia , Flooding in Karawang ( West Java ) - Indonesia\n",
      "\n",
      "Original: Mike Lester for February 19 , 2024\n",
      "Cleaned : Mike Lester for\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# def remove_structured_dates(text):\n",
    "#\n",
    "#     # Month variations (short and full), case insensitive\n",
    "#     months = r\"(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|\" \\\n",
    "#              r\"Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\"\n",
    "\n",
    "#     # Patterns to match structured dates (with hyphens or spaces)\n",
    "#     patterns = [\n",
    "#         rf\"\\b\\d{{1,2}}\\s*[-/]\\s*{months}\\s*[-/]\\s*\\d{{4}}\\b\",  # 18 - Jan - 2024\n",
    "#         rf\"\\b\\d{{1,2}}\\s*{months}\\s*,?\\s*\\d{{4}}\\b\",           # 18 Jan 2024 or 18 Jan, 2024\n",
    "#         rf\"\\b{months}\\s*\\d{{1,2}}\\s*,?\\s*\\d{{4}}\\b\",           # Jan 18, 2024\n",
    "#         rf\"\\b\\d{{1,2}}(st|nd|rd|th)?\\s*{months}\\s*,?\\s*\\d{{4}}\\b\",  # 18th January, 2024\n",
    "#     ]\n",
    "\n",
    "#     for p in patterns:\n",
    "#         text = re.sub(p, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Clean up extra spaces or leftover punctuation\n",
    "#     text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "#     return text.strip(\" ,;-\")\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_structured_dates(text):\n",
    "\n",
    "    months = r\"(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?\" \\\n",
    "             r\"|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\"\n",
    "\n",
    "    weekdays = r\"(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\"\n",
    "\n",
    "    patterns = [\n",
    "        # Remove full date formats\n",
    "        rf\"\\b\\d{{1,2}}\\s+{months}\\s+\\d{{4}}\\b\",                              # e.g. 29 January 2024\n",
    "        rf\"\\b{months}\\s+\\d{{1,2}}(?:st|nd|rd|th)?\\s*,?\\s*\\d{{4}}\\b\",         # e.g. January 30, 2024\n",
    "        rf\"\\(\\s*\\d{{1,2}}\\s+{months}\\s+\\d{{4}}\\s*\\)\",                        # e.g. (31 Jan 2024)\n",
    "        rf\"\\b{months}\\s+\\d{{1,2}}(?:st|nd|rd|th)?\\s+\\d{{4}}\\b\",              # e.g. January 31st 2024\n",
    "        r\"\\b\\d{1,2}\\s*:\\s*\\d{2}\\s*UTC\\b\",                                    # e.g. 16 : 50 UTC\n",
    "\n",
    "        # Remove standalone weekday names (with optional comma)\n",
    "        rf\"\\b{weekdays},?\\b\",                                               # e.g. Tuesday\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Clean up spacing and punctuation\n",
    "    text = re.sub(r'\\s*,\\s*,+', ', ', text)\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = re.sub(r'\\s*([,;:\\-])\\s*', r' \\1 ', text)\n",
    "    return text.strip(\" ,;- \")\n",
    "\n",
    "\n",
    "\n",
    "# Test cases\n",
    "titles = [\n",
    "    \"World Earthquake Report for Monday , 29 January 2024\",\n",
    "    \"HAWAIIAN VOLCANO OBSERVATORY DAILY UPDATE Tuesday , January 30 , 2024 , 16 : 50 UTC \",\n",
    "    \"Vanuatu Volcano Alert Bulletin nÂ°3 - Ambrym Activity ( January 31st 2024 ) - Vanuatu\",\n",
    "    \"Indonesia , Flooding in Karawang ( West Java ) ( 31 Jan 2024 ) - Indonesia\",\n",
    "    \"Mike Lester for February 19 , 2024\"\n",
    "\n",
    "]\n",
    "\n",
    "cleaned = [remove_structured_dates(t) for t in titles]\n",
    "for original, result in zip(titles, cleaned):\n",
    "    print(f\"Original: {original}\\nCleaned : {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d44e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c31a4e97",
   "metadata": {},
   "source": [
    "### 2.5 - Handle Acronmys\n",
    "Acronyms are shortened forms of phrases, generally found in informal writings. For the sake of proper modeling, we convert the acronyms, appearing in the titles, back to their respective original forms.\n",
    "- Example:\n",
    "- Fyi -> For Your Information\n",
    "- ASAP -> As Soon As Possible\n",
    "- btw -> by the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518548e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of acronyms\n",
    "\n",
    "# Most common acronmyms used in social media is available in this json file\n",
    "acronyms_url = 'https://raw.githubusercontent.com/ShravanTV/Natural_Catastrophe_Events/refs/heads/main/abbrevations.json'\n",
    "acronyms_dict = pd.read_json(acronyms_url, typ = 'series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59008f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Original form of the acronym 'fyi' is 'for your information'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acronym</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aka</td>\n",
       "      <td>also known as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asap</td>\n",
       "      <td>as soon as possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brb</td>\n",
       "      <td>be right back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>btw</td>\n",
       "      <td>by the way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dob</td>\n",
       "      <td>date of birth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  acronym             original\n",
       "0     aka        also known as\n",
       "1    asap  as soon as possible\n",
       "2     brb        be right back\n",
       "3     btw           by the way\n",
       "4     dob        date of birth"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example: Original form of the acronym 'fyi' is '{}'\".format(acronyms_dict[\"fyi\"]))\n",
    "\n",
    "# Function to convert a given dictionary into a dataframe with given column names\n",
    "def dict_to_df(dictionary, C1, C2):\n",
    "    df = pd.DataFrame(dictionary.items(), columns=[C1, C2])\n",
    "    return df\n",
    "    \n",
    "# Dataframe of acronyms\n",
    "dict_to_df(acronyms_dict, \"acronym\", \"original\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of acronyms\n",
    "acronyms_list = list(acronyms_dict.keys())\n",
    "\n",
    "# RegexpTokenizer\n",
    "regexp = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Function to convert contractions in a text\n",
    "def convert_acronyms(text):\n",
    "    words = []\n",
    "    text = text.lower()\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_list:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15d97438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Homeowners affected by Hurricane Helene advised to file insurance claims ASAP\n",
      "Output: homeowners affected by hurricane helene advised to file insurance claims as soon as possible\n"
     ]
    }
   ],
   "source": [
    "text = \"Homeowners affected by Hurricane Helene advised to file insurance claims ASAP\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_acronyms(text)))\n",
    "\n",
    "# Can be seen asap is converted to \"as soon as possible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1575a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c7d3ae9",
   "metadata": {},
   "source": [
    "### 2.6 - Handle Contractions\n",
    "A contraction is a shortened form of a word or a phrase, obtained by dropping one or more letters.\n",
    "- Examples: \n",
    "- aren't -> are not\n",
    "- wasn't -> was not\n",
    "- arent -> are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a80f90d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Original form of the contraction 'aren't' is 'was not'\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of contractions\n",
    "contractions_url = 'https://raw.githubusercontent.com/ShravanTV/Natural_Catastrophe_Events/refs/heads/main/Contractions_lowercase.json'\n",
    "contractions_dict = pd.read_json(contractions_url, typ = 'series')\n",
    "\n",
    "print(\"Example: Original form of the contraction 'aren't' is '{}'\".format(contractions_dict[\"wasnt\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45576cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contraction</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'aight</td>\n",
       "      <td>alright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ain't</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amn't</td>\n",
       "      <td>am not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arencha</td>\n",
       "      <td>are not you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aren't</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  contraction     original\n",
       "0      'aight      alright\n",
       "1       ain't      are not\n",
       "2       amn't       am not\n",
       "3     arencha  are not you\n",
       "4      aren't      are not"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe of contractions\n",
    "dict_to_df(contractions_dict, \"contraction\", \"original\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f723cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of contractions\n",
    "contractions_list = list(contractions_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert contractions in a text\n",
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ba98d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Most people arent insured for the worst\n",
      "Output: Most people are not insured for the worst\n",
      "\n",
      "Input: I wasnt aware of the situation\n",
      "Output: I was not aware of the situation\n"
     ]
    }
   ],
   "source": [
    "text = \"Most people arent insured for the worst\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_contractions(text)))\n",
    "\n",
    "text = \"I wasnt aware of the situation\"\n",
    "print(\"\\nInput: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_contractions(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d092553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "083597db",
   "metadata": {},
   "source": [
    "### 2.7 - Spelling Checker and correct misspelled words (Not used as its creating a lot of noise)\n",
    "- Use spellchecker to correct misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f5d6bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I'm goinng therre\n",
      "Output: I m going there\n"
     ]
    }
   ],
   "source": [
    "# Spelling checker\n",
    "\n",
    "# pyspellchecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def pyspellchecker(text):\n",
    "    word_list = regexp.tokenize(text)\n",
    "    word_list_corrected = []\n",
    "    for word in word_list:\n",
    "        if word in spell.unknown(word_list):\n",
    "            word_list_corrected.append(spell.correction(word))\n",
    "        else:\n",
    "            word_list_corrected.append(word)\n",
    "    text_corrected = \" \".join(word_list_corrected)\n",
    "    return text_corrected\n",
    "\n",
    "text = \"I'm goinng therre\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(pyspellchecker(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f65539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6901e38",
   "metadata": {},
   "source": [
    "### 2.8 - Remove Special Characters from the title column\n",
    "- Remove special characters (keep alphanumeric and spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fc9ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: DSWD DROMIC Report # 1 on the Tornado Incident in Brgy . Candating , Arayat , Pampanga as of 01 June 2024 , 6AM - Philippines\n",
      "Output: DSWD DROMIC Report  1 on the Tornado Incident in Brgy  Candating  Arayat  Pampanga as of 01 June 2024  6AM  Philippines\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove special characters (keep alphanumeric and spaces)\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "text =\"DSWD DROMIC Report # 1 on the Tornado Incident in Brgy . Candating , Arayat , Pampanga as of 01 June 2024 , 6AM - Philippines\"\n",
    "\n",
    "# All special characters except apostraphe should be removed\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_special_characters(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9855c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e207e5",
   "metadata": {},
   "source": [
    "\n",
    "### 2.9 - Remove Extra whitespaces in the title\n",
    "- Remove extra whitespaces in the title\n",
    "- Remove leading and trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d864aa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  govt wasnt prepared for  unprecedented condition in 2023 wildfire season , review finds\n",
      "Output: govt wasnt prepared for unprecedented condition in 2023 wildfire season , review finds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Removing extra whitespace\n",
    "def remove_extra_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "text = \" govt wasnt prepared for  unprecedented condition in 2023 wildfire season , review finds\"\n",
    "\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_extra_whitespace(text)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da679c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "562562a0",
   "metadata": {},
   "source": [
    "### 2.10 Remove Emojis from the data if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da698052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Just happened a terrible car crash 😟\n",
      "Output: Just happened a terrible car crash \n"
     ]
    }
   ],
   "source": [
    "# Removing emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "text1 = \"Just happened a terrible car crash 😟\"\n",
    "print(\"Input: {}\".format(text1))\n",
    "print(\"Output: {}\".format(remove_emoji(text1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412eedde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9ac5f74",
   "metadata": {},
   "source": [
    "### As Part of cleaning data following steps should be performed\n",
    "\n",
    "1. Apply Basic cleaning of titles using above discussed functions.\n",
    "2. Identify natural catastrophe events using transformer model and custom logic.\n",
    "3. Filter out non natural catastrophe events.\n",
    "4. Lemmatize basic cleaned titles, remove stop words and non alphabetic characters and locations.\n",
    "5. Save cleaned data which will be used for further analysis.\n",
    "\n",
    "I have created a script with multiprocessing containing pipeline to basic clean, detect Nat-Cat and advanced clean (stop word removal, lemmatisation, location removal) and is available in `scripts\\Clean_data.py` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a88513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a4f633",
   "metadata": {},
   "source": [
    "## 3. Apply Basic cleaning on Nat Cat Events CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0c345",
   "metadata": {},
   "source": [
    "As cleaning data with all the above discussed functions takes time, I have implemented a multiprocessing script to handle these tasks.\n",
    "\n",
    "Execute `scripts\\Clean_data.py` file, which produces 3 CSV files inside `data` folder\n",
    "\n",
    "- `1. Events cleaned with basic functions` - Contains title after applying functions discussed in above steps.\n",
    "- `2. NatCat events` - Contains column with True/False, stating if title is a Nat-Cat event or not.\n",
    "- `3. Fully Cleaned events with lemmatised title` - Fully cleaned file containing lemmatised_title obtained after removing stop words and location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6511a23c",
   "metadata": {},
   "source": [
    "Now, load and analyse data which is cleaned using above discussed 10 functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca8d757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Basic Cleaned data\n",
    "df = pd.read_csv(\"../data/1. Events cleaned with basic functions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a2d01eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65158 entries, 0 to 65157\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   url            65158 non-null  object\n",
      " 1   url_mobile     19704 non-null  object\n",
      " 2   title          65158 non-null  object\n",
      " 3   seendate       65158 non-null  object\n",
      " 4   socialimage    56494 non-null  object\n",
      " 5   domain         65158 non-null  object\n",
      " 6   language       65158 non-null  object\n",
      " 7   sourcecountry  64022 non-null  object\n",
      " 8   cleaned_title  65153 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640051b9",
   "metadata": {},
   "source": [
    "Can be seen a total of 65158 rows in the dataset.\n",
    "This is because duplicate titles has been removed from the data during cleaning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "387df0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2023 was a year of extreme weather in southern...\n",
       "1        hawaiian volcano observatory daily update usgs...\n",
       "2                how to protect your family from tornadoes\n",
       "3        iceland volcanoes bring tourists to island cou...\n",
       "4        tornados scorchers and ice storm top 10 weathe...\n",
       "                               ...                        \n",
       "65153    indonesia mount ibu erupts on 2024 last day vi...\n",
       "65154    montgomery county crime authorities detain dri...\n",
       "65155    love island india reynolds reveals her family ...\n",
       "65156    new year celebrated across world but united ki...\n",
       "65157    kentucky humane society receives 30 000 grant ...\n",
       "Name: cleaned_title, Length: 65158, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the cleaned title column\n",
    "df['cleaned_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466fcbf",
   "metadata": {},
   "source": [
    "Now the column `cleaned_title` is cleaned and can be used to detect if its a Natural Catastrophic event related title or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5370307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d01453b",
   "metadata": {},
   "source": [
    "## 4. Check if event is a Natural Catastrophe Event\n",
    "\n",
    "Criteria to mark a event as Nat-Cat event:\n",
    "-\tNatural Catastrophic disaster should have occurred\n",
    "-\tMust contain a location.\n",
    "-\tMust represent an event that has occurred.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708b813",
   "metadata": {},
   "source": [
    "\n",
    "In order to detect a Nat-Cat event based on above criteria, I used a hybrid approach\n",
    "\n",
    "- Used pre-trained transformer model to detect if title contains natural catastrophe event\n",
    "- Used Spacy based transformer model to detect locations from the title.\n",
    "- Used custom and Spacy based transformer model to check if title have any past tense, past participle, present 3rd person related words.\n",
    "\n",
    "Finally, if all the above conditions are met, then marked the title as a natural catastrophe event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a295ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4c11a",
   "metadata": {},
   "source": [
    "Details of the models used\n",
    "\n",
    "1. Used `hannybal/disaster-twitter-xlm-roberta-al` model to identify disaster :\n",
    "    * This model leverages a fine-tuned version of the multilingual XLM-RoBERTa architecture, specifically trained on crisis/event-related tweets. It uses the tokenizer from cardiffnlp/twitter-xlm-roberta-base for handling social media text and is fine-tuned on disaster-related data to classify sentences accordingly. The model supports multiple languages, making it suitable for global disaster monitoring tasks. And therefore used in this task to detect if any disaster related words available in the titles.\n",
    "2. Used `en_core_web_trf` model from spaCy: \n",
    "    * This is a transformer-based English NLP pipeline built on top of pretrained transformer models like RoBERTa. It provides accurate named entity recognition (NER), including the ability to detect geographic locations (GPE, LOC) in text. And using this model I am detecting if title contains location.\n",
    "3. Detect past tense: \n",
    "    * To identify whether a disaster event has occurred in the past, a rule-based method was used alongside spaCy's transformer model. The approach first checks for predefined past-tense indicator verbs (e.g., \"struck\", \"occurred\", \"killed\") using lemmatized tokens. If none are found, it falls back to checking part-of-speech tags for past-tense verb forms (e.g., VBD, VBN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spacy transformer-based model to detect locations and past tense from the titles\n",
    "nlp = spacy.load(\"en_core_web_trf\")  \n",
    "\n",
    "\n",
    "# Pretrained model for disaster-related detection\n",
    "model_name = \"hannybal/disaster-twitter-xlm-roberta-al\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Labels for the disaster model predictions \n",
    "LABELS = {0: \"non-disaster\", 1: \"disaster\"}\n",
    "\n",
    "\n",
    "# Exclusion terms for filtering out non-disaster events\n",
    "# Sometimes titles contain exercise, drill, mock, test etc. which are not real disasters and should be excluded.\n",
    "EXCLUSION_TERMS = {\n",
    "    'drill', 'exercise', 'simulation', 'mock', 'test'\n",
    "}\n",
    "\n",
    "# \n",
    "NATURAL_CATASTROPHE_TYPES = {\n",
    "    'earthquake', 'flood', 'lava', 'volcano', 'eruption', 'wildfire',\n",
    "    'tornado', 'cyclone', 'hurricane', 'typhoon', 'tsunami',\n",
    "    'landslide', 'drought', 'storm', 'blizzard', 'avalanche',\n",
    "    'heatwave', 'lightning', 'quake', 'storm'\n",
    "}\n",
    "\n",
    "# Function to check if a title is related to a disaster using the pretrained model\n",
    "def is_disaster_title(text, threshold=0.7, verbose=True):\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**tokenized).logits\n",
    "        probs = F.softmax(logits, dim=-1) # Disaster model output probabilities in float format and using softmax to convert flats to real probabilities\n",
    "        score, pred = torch.max(probs, dim=1)\n",
    "    \n",
    "    is_dis = (pred.item() == 1 and score.item() >= threshold)\n",
    "    if verbose:\n",
    "        print(f\"Title: {text}\")\n",
    "        print(f\"Prediction: {LABELS[pred.item()]}, Confidence: {score.item():.2f}\")\n",
    "    return is_dis\n",
    "\n",
    "\n",
    "# Function to check if a title is related to a natural catastrophe event\n",
    "def is_nat_cat_event(title):\n",
    "    \"\"\"\"\n",
    "    Function to check if a title is related to a natural catastrophe event based on criteria.\n",
    "    - Checks if title contains any exercises, drills, mock, test etc. which are not real disasters and returns False.\n",
    "    - Checks for Natural Catastrophe disasters using a pretrained transformer model and through custom keyword, if both returns says no Nat-Cat event then returns False.\n",
    "    - Checks if the title contains any location entities using spaCy.\n",
    "    - Checks if the title indicates a past or present tense event.\n",
    "\n",
    "    \"\"\"\n",
    "    doc = nlp(title)\n",
    "\n",
    "    disaster_detection_model = True\n",
    "    disaster_detection_custom = True\n",
    "\n",
    "    # Convert title to lowercase for case-insensitive matching\n",
    "    lower_title = title.lower()\n",
    "\n",
    "    # If any exclusion terms is present, return False\n",
    "    if any(term in lower_title for term in EXCLUSION_TERMS):\n",
    "        print(\"Filtered due to exclusion term\")\n",
    "        return False\n",
    "\n",
    "    # If disaster not detected through transformer model then set disaster_detection_model to False\n",
    "    if not is_disaster_title(lower_title):\n",
    "        print(\"Natural catastrophe event not detected through transformer model.\")\n",
    "        disaster_detection_model = False\n",
    "\n",
    "    # Custom keyword check for natural catastrophe event\n",
    "    if not any(word in lower_title for word in NATURAL_CATASTROPHE_TYPES):\n",
    "        print(\"Natural catastrophe event not detected in custom keyword.\")\n",
    "        disaster_detection_custom = False\n",
    "\n",
    "\n",
    "    # Model should identify disaster or title should have custom keyword related to natural catastrophe\n",
    "    # If any of the above conditions are not met, return False else return True\n",
    "    if not disaster_detection_model or not disaster_detection_custom:\n",
    "        return False\n",
    "\n",
    "\n",
    "    # Rule-based: check for location using spaCy NER\n",
    "    has_location = any(ent.label_ in ['GPE', 'LOC'] for ent in doc.ents)\n",
    "    print(\"Contain location : \",has_location)\n",
    "\n",
    "\n",
    "    # Rule-based: check for past/present-tense event (for better precision)\n",
    "    past_event_indicators = {\n",
    "        'struck', 'hit', 'occurred', 'erupted', 'caused', 'killed',\n",
    "        'damaged', 'destroyed', 'swept', 'triggered', 'sparked',\n",
    "        'flooded', 'burned', 'ravaged', 'wreaked', 'devastated',\n",
    "        'reported', 'identified'\n",
    "    }\n",
    "\n",
    "    past_event = any(token.lemma_.lower() in past_event_indicators for token in doc)\n",
    "    print(\"Custom past event present : \",past_event)\n",
    "\n",
    "    # If past_event is not detected then check for past_event using token tag\n",
    "    if not past_event:\n",
    "        past_event = any(token.tag_ in ['VBD', 'VBN', 'VBZ'] for token in doc)\n",
    "        print(\"Token past event detected : \",past_event)\n",
    "\n",
    "    # If any of the above conditions are not met, return False else return True\n",
    "    return has_location and past_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b5c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: new state report : wildfire smoke increased death rate in spokane , across washington\n",
      "Prediction: disaster, Confidence: 1.00\n",
      "Contain location :  True\n",
      "Custom past event present :  False\n",
      "Token past event detected :  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "is_nat_cat_event(\"New state report : Wildfire smoke increased death rate in Spokane , across Washington\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a5cb2",
   "metadata": {},
   "source": [
    "As discussed earlier executing `scripts\\Clean_data.py` applies above Nat-Cat detection function to data  and generates a file.\n",
    "Lets load the Nat-cat detected file and see few results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "244d512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading already processed Nat-Cat data file\n",
    "df = pd.read_csv(\"../data/2. NatCat events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70f8c08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>url_mobile</th>\n",
       "      <th>title</th>\n",
       "      <th>seendate</th>\n",
       "      <th>socialimage</th>\n",
       "      <th>domain</th>\n",
       "      <th>language</th>\n",
       "      <th>sourcecountry</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>is_natcat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.wpri.com/weather/severe-weather/20...</td>\n",
       "      <td>https://www.wpri.com/weather/severe-weather/20...</td>\n",
       "      <td>2023 was a year of extreme weather in Southern...</td>\n",
       "      <td>20240101T223000Z</td>\n",
       "      <td>https://www.wpri.com/wp-content/uploads/sites/...</td>\n",
       "      <td>wpri.com</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>2023 was a year of extreme weather in southern...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://volcanoes.usgs.gov/hans2/view/notice/D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HAWAIIAN VOLCANO OBSERVATORY DAILY UPDATE Mond...</td>\n",
       "      <td>20240101T220000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>volcanoes.usgs.gov</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>hawaiian volcano observatory daily update usgs...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.ktbs.com/online_features/home_impr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How to Protect Your Family from Tornadoes</td>\n",
       "      <td>20240101T124500Z</td>\n",
       "      <td>https://bloximages.newyork1.vip.townnews.com/k...</td>\n",
       "      <td>ktbs.com</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>how to protect your family from tornadoes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.ctvnews.ca/climate-and-environment...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Iceland volcanoes bring tourists to island cou...</td>\n",
       "      <td>20240101T223000Z</td>\n",
       "      <td>https://www.ctvnews.ca/content/dam/ctvnews/en/...</td>\n",
       "      <td>ctvnews.ca</td>\n",
       "      <td>English</td>\n",
       "      <td>Canada</td>\n",
       "      <td>iceland volcanoes bring tourists to island cou...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://news.yahoo.com/tornados-scorchers-ice-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tornados , scorchers and ice storm : Top 10 we...</td>\n",
       "      <td>20240101T131500Z</td>\n",
       "      <td>https://s.yimg.com/ny/api/res/1.2/PXdWVXp40q9s...</td>\n",
       "      <td>news.yahoo.com</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>tornados scorchers and ice storm top 10 weathe...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.wpri.com/weather/severe-weather/20...   \n",
       "1  https://volcanoes.usgs.gov/hans2/view/notice/D...   \n",
       "2  https://www.ktbs.com/online_features/home_impr...   \n",
       "3  https://www.ctvnews.ca/climate-and-environment...   \n",
       "4  https://news.yahoo.com/tornados-scorchers-ice-...   \n",
       "\n",
       "                                          url_mobile  \\\n",
       "0  https://www.wpri.com/weather/severe-weather/20...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               title          seendate  \\\n",
       "0  2023 was a year of extreme weather in Southern...  20240101T223000Z   \n",
       "1  HAWAIIAN VOLCANO OBSERVATORY DAILY UPDATE Mond...  20240101T220000Z   \n",
       "2          How to Protect Your Family from Tornadoes  20240101T124500Z   \n",
       "3  Iceland volcanoes bring tourists to island cou...  20240101T223000Z   \n",
       "4  Tornados , scorchers and ice storm : Top 10 we...  20240101T131500Z   \n",
       "\n",
       "                                         socialimage              domain  \\\n",
       "0  https://www.wpri.com/wp-content/uploads/sites/...            wpri.com   \n",
       "1                                                NaN  volcanoes.usgs.gov   \n",
       "2  https://bloximages.newyork1.vip.townnews.com/k...            ktbs.com   \n",
       "3  https://www.ctvnews.ca/content/dam/ctvnews/en/...          ctvnews.ca   \n",
       "4  https://s.yimg.com/ny/api/res/1.2/PXdWVXp40q9s...      news.yahoo.com   \n",
       "\n",
       "  language  sourcecountry                                      cleaned_title  \\\n",
       "0  English  United States  2023 was a year of extreme weather in southern...   \n",
       "1  English  United States  hawaiian volcano observatory daily update usgs...   \n",
       "2  English  United States          how to protect your family from tornadoes   \n",
       "3  English         Canada  iceland volcanoes bring tourists to island cou...   \n",
       "4  English  United States  tornados scorchers and ice storm top 10 weathe...   \n",
       "\n",
       "   is_natcat  \n",
       "0      False  \n",
       "1      False  \n",
       "2      False  \n",
       "3      False  \n",
       "4      False  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "326a81c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_natcat\n",
       "False    44337\n",
       "True     18490\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_natcat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6d44f",
   "metadata": {},
   "source": [
    "- Out of 62827 titles, 18490 titles are identified as Nat-cat events\n",
    "- Which is around 30% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5800adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 was a year of extreme weather in southern new england\n",
      "hawaiian volcano observatory daily update usgs hazard notification system hans for volcanoes\n",
      "how to protect your family from tornadoes\n",
      "iceland volcanoes bring tourists to island country\n",
      "tornados scorchers and ice storm top 10 weather events in abilene and san angelo areas\n",
      "bumeran house lucas maino fernandez\n",
      "oggy oggy channel 5 hd tvguide co uk\n",
      "senior military leader canadians overly comfortable as global security shifts\n",
      "beyond the barometer a look back at a year in weather\n",
      "go jetters cbeebies tvguide co uk\n"
     ]
    }
   ],
   "source": [
    "# Observe few Non-Catastrophe events excluded by our function\n",
    "for title in df.loc[df['is_natcat'] == False, 'cleaned_title'].head(10).values:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efdbb7e",
   "metadata": {},
   "source": [
    "- None of the above titles are related to disasters which we are interested.\n",
    "- Although title \"2023 was a year of extreme weather in southern new england\", contains location, past tense and keyword 'extreme weather', its not actaully a disaster, so model didnt classified this as a Nat-Cat event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155ee14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ca1c81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dickinson hit hardest by tornadoes weather service says\n",
      "live israel continues to block gaza aid heavy rains flood tent camps\n",
      "flood committee disburses n18bn to 101 330 borno households\n",
      "flooding causes travel chaos in the north and north east on new year eve\n",
      "floods more evacuated in johor situation improves in kelantan\n",
      "borno flood committee disburses n18 billion to 101 330 households\n",
      "wild weather and storm chaos puts dampener on scotland world famous hogmanay party\n",
      "north coast braces for potential floods as more rain is forecasted this week\n",
      "montgomery county crime authorities detain driver while patrolling area affected by tornado\n",
      "love island india reynolds reveals her family we are separated after being caught in the 2004 boxing day tsunami it shall never sink in how lucky we we are\n"
     ]
    }
   ],
   "source": [
    "# Observe few Catastrophe events detected by our function\n",
    "for title in df.loc[df['is_natcat'] == True, 'cleaned_title'].tail(10).values:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e1424",
   "metadata": {},
   "source": [
    "- Can be observed that almost all the titles are related to Nat-Cat events and contains location and past tense showing event occured.\n",
    "- From this we can say our function is correctly identifying disaster related events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e41eb5",
   "metadata": {},
   "source": [
    "Now we can filter out Non Not-Cat events and apply Final cleaning on Nat-Cat events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e98b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d4ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54ade963",
   "metadata": {},
   "source": [
    "## 5. Final Cleaning \n",
    "- Filter out Non-NatCat events and apply final cleaning.\n",
    "- Apply Lemmatisation to reduce words to their base form on basic cleaned data\n",
    "- Define custom stop words which should not be removed as they add value to the text\n",
    "- Remove Non- alphabetic words as they are not relevant to the text.\n",
    "- Remove locations from the title as they are not anymore needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "228f42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk import regexp_tokenize\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Custom stopwords – keep location/disaster-relevant prepositions\n",
    "custom_stopwords = STOPWORDS.difference({\n",
    "    'after', 'before', 'during', 'against', 'under', 'near', 'over',\n",
    "    'between', 'while', 'within', 'through', 'until', 'without',\n",
    "    \"earthquake\", \"quake\", \"earthquakes\",\n",
    "    \"flood\",\"floods\",\"flooding\", \"landslide\", \"landslides\", \"avalanche\", \"blizzard\", \"tide\", \"drought\", \"inundation\", \"deluge\", \"tsunami\", \"river\",\n",
    "    \"tornado\", \"tornadoes\", \"storm\", \"hurricane\", \"cyclone\", \"typhoon\", \"lightning\", \"heatwave\", \"twister\", \"funnelcloud\",\n",
    "    \"volcano\", \"volcanoes\", \"eruption\", \"lava\", \"ash\",\n",
    "    \"wildfire\", \"wildfire\", \"wildfires\", \"fire\",\"bushfire\", \"forestfire\"\n",
    "})\n",
    "\n",
    "# Step 1: Lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "# Step 2: Remove stopwords AFTER lemmatization\n",
    "def remove_custom_stopwords(text):\n",
    "    return ' '.join(word for word in text.split() if word not in custom_stopwords)\n",
    "\n",
    "# Step 3: Remove non-alphabetic words (no digits, punctuation, etc.)\n",
    "def discard_non_alpha(text):\n",
    "    word_list_non_alpha = [word for word in regexp_tokenize(text, pattern=r'\\w+|\\$[\\d\\.]+|\\S+') if word.isalpha()]\n",
    "    return \" \".join(word_list_non_alpha)\n",
    "\n",
    "# Step 4: Remove locations from the titles\n",
    "def remove_locations(text):\n",
    "    \"\"\"Remove location entities from text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    # Keep tokens that are not location entities\n",
    "    tokens = [token.text for token in doc if not token.ent_type_ in ['GPE', 'LOC']]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Combine all steps into one function\n",
    "def preprocess_text(text):\n",
    "    lemmatized = lemmatize_text(text)\n",
    "    no_stopwords = remove_custom_stopwords(lemmatized)\n",
    "    clean_text = discard_non_alpha(no_stopwords)\n",
    "    no_locations = remove_locations(clean_text)\n",
    "    return no_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a654c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 4.0 Magnitude Earthquake Reported In Japan\n",
      "Processed: magnitude earthquake report\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sample = \"4.0 Magnitude Earthquake Reported In Japan\"\n",
    "processed = preprocess_text(sample)\n",
    "\n",
    "print(\"Original:\", sample)\n",
    "print(\"Processed:\", processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59096b41",
   "metadata": {},
   "source": [
    "This cleaning step is part of pipeline created and executing `scripts\\Clean_data.py` cleans and provides final cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bebcd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the cleaned data with lemmatized titles\n",
    "df = pd.read_csv(\"../data/3. Fully Cleaned events with lemmatised title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2e8da70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>url_mobile</th>\n",
       "      <th>title</th>\n",
       "      <th>seendate</th>\n",
       "      <th>socialimage</th>\n",
       "      <th>domain</th>\n",
       "      <th>language</th>\n",
       "      <th>sourcecountry</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>is_natcat</th>\n",
       "      <th>lemmatised_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://globalnews.ca/news/10198334/japan-eart...</td>\n",
       "      <td>https://globalnews.ca/news/10198334/japan-eart...</td>\n",
       "      <td>Japan earthquakes : Coastal residents told to ...</td>\n",
       "      <td>20240101T161500Z</td>\n",
       "      <td>https://globalnews.ca/wp-content/uploads/2024/...</td>\n",
       "      <td>globalnews.ca</td>\n",
       "      <td>English</td>\n",
       "      <td>Canada</td>\n",
       "      <td>japan earthquakes coastal residents told to ev...</td>\n",
       "      <td>True</td>\n",
       "      <td>earthquake coastal resident tell evacuate amid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.columbian.com/news/2023/dec/31/new...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New state report : Wildfire smoke increased de...</td>\n",
       "      <td>20240101T053000Z</td>\n",
       "      <td>https://pcdn.columbian.com/wp-content/themes/c...</td>\n",
       "      <td>columbian.com</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>new state report wildfire smoke increased deat...</td>\n",
       "      <td>True</td>\n",
       "      <td>new state report wildfire smoke increase death...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.forbes.com/sites/jamiecartereurope...</td>\n",
       "      <td>https://www.forbes.com/sites/jamiecartereurope...</td>\n",
       "      <td>In Photos : NASA Juno Flies Just 930 Miles Abo...</td>\n",
       "      <td>20240101T013000Z</td>\n",
       "      <td>https://imageio.forbes.com/specials-images/ima...</td>\n",
       "      <td>forbes.com</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>in photos nasa juno flies just 930 miles above...</td>\n",
       "      <td>True</td>\n",
       "      <td>photo nasa juno fly mile volcano jupiter viole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.wishtv.com/weather/weather-stories...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023 finishes as 3rd warmest in central Indian...</td>\n",
       "      <td>20240101T180000Z</td>\n",
       "      <td>https://www.wishtv.com/wp-content/uploads/2024...</td>\n",
       "      <td>wishtv.com</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>2023 finishes as 3rd warmest in central indian...</td>\n",
       "      <td>True</td>\n",
       "      <td>finish warm central tornado statewide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.cambridge-news.co.uk/news/local-ne...</td>\n",
       "      <td>https://www.cambridge-news.co.uk/news/local-ne...</td>\n",
       "      <td>New Year Day flood alerts issued in Cambridges...</td>\n",
       "      <td>20240101T124500Z</td>\n",
       "      <td>https://i2-prod.cambridge-news.co.uk/incoming/...</td>\n",
       "      <td>cambridge-news.co.uk</td>\n",
       "      <td>English</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>new year day flood alerts issued in cambridges...</td>\n",
       "      <td>True</td>\n",
       "      <td>new year day flood alert issue cambridgeshire ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://globalnews.ca/news/10198334/japan-eart...   \n",
       "1  https://www.columbian.com/news/2023/dec/31/new...   \n",
       "2  https://www.forbes.com/sites/jamiecartereurope...   \n",
       "3  https://www.wishtv.com/weather/weather-stories...   \n",
       "4  https://www.cambridge-news.co.uk/news/local-ne...   \n",
       "\n",
       "                                          url_mobile  \\\n",
       "0  https://globalnews.ca/news/10198334/japan-eart...   \n",
       "1                                                NaN   \n",
       "2  https://www.forbes.com/sites/jamiecartereurope...   \n",
       "3                                                NaN   \n",
       "4  https://www.cambridge-news.co.uk/news/local-ne...   \n",
       "\n",
       "                                               title          seendate  \\\n",
       "0  Japan earthquakes : Coastal residents told to ...  20240101T161500Z   \n",
       "1  New state report : Wildfire smoke increased de...  20240101T053000Z   \n",
       "2  In Photos : NASA Juno Flies Just 930 Miles Abo...  20240101T013000Z   \n",
       "3  2023 finishes as 3rd warmest in central Indian...  20240101T180000Z   \n",
       "4  New Year Day flood alerts issued in Cambridges...  20240101T124500Z   \n",
       "\n",
       "                                         socialimage                domain  \\\n",
       "0  https://globalnews.ca/wp-content/uploads/2024/...         globalnews.ca   \n",
       "1  https://pcdn.columbian.com/wp-content/themes/c...         columbian.com   \n",
       "2  https://imageio.forbes.com/specials-images/ima...            forbes.com   \n",
       "3  https://www.wishtv.com/wp-content/uploads/2024...            wishtv.com   \n",
       "4  https://i2-prod.cambridge-news.co.uk/incoming/...  cambridge-news.co.uk   \n",
       "\n",
       "  language   sourcecountry                                      cleaned_title  \\\n",
       "0  English          Canada  japan earthquakes coastal residents told to ev...   \n",
       "1  English   United States  new state report wildfire smoke increased deat...   \n",
       "2  English   United States  in photos nasa juno flies just 930 miles above...   \n",
       "3  English   United States  2023 finishes as 3rd warmest in central indian...   \n",
       "4  English  United Kingdom  new year day flood alerts issued in cambridges...   \n",
       "\n",
       "   is_natcat                                   lemmatised_title  \n",
       "0       True  earthquake coastal resident tell evacuate amid...  \n",
       "1       True  new state report wildfire smoke increase death...  \n",
       "2       True  photo nasa juno fly mile volcano jupiter viole...  \n",
       "3       True              finish warm central tornado statewide  \n",
       "4       True  new year day flood alert issue cambridgeshire ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0149c1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18490, 11)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86251d9",
   "metadata": {},
   "source": [
    "Can see a total of 18490 Nat-Cat related events are identified and cleaned (All the steps discussed till now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90537be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dickinson hit hard tornado weather service\n",
      "live continue block aid heavy rain flood tent camp\n",
      "flood committee disburse borno household\n",
      "flooding cause travel chaos new year eve\n",
      "flood evacuate johor situation improve\n",
      "borno flood committee disburse billion household\n",
      "wild weather storm chaos dampener scotland world famous hogmanay party\n",
      "brace potential flood rain forecast week\n",
      "crime authority detain driver while patrolling area affect tornado\n",
      "love island reveal family separate after catch boxing day tsunami shall sink lucky\n"
     ]
    }
   ],
   "source": [
    "# Observe few lemmatised titles \n",
    "for title in df['lemmatised_title'].tail(10).values:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b046e90",
   "metadata": {},
   "source": [
    "Can see `lemmatised_title` column in dataframe, is now fully cleaned containing rich lemmatised data, which can be used in next steps for Feature engineering and Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fdb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7ed90b4",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- In Step 1: Removed noise from the title column by performing cleaning operations, where we removed structured dates, URLs, HTML, emojis, special characters, extra whitespace, acronyms, and contractions etc.\n",
    "- In Step 2: Detected natural catastrophe events using the NatCatEventDetector class and filtered the natural catastrophe events and saved the results.\n",
    "- In Step 3: Lemmatized, removed stop words, locations and non-alphabetic words from the titles in and saved the results.\n",
    "\n",
    "\n",
    "This final output containing cleaned data with `lemmatised_title` column is saved in the file `3. Fully Cleaned events with lemmatised title.csv` and is used as input for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7632e300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe58049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_brightflag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
